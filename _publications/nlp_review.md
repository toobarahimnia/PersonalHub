---
title: "From RNNs to Transformers: A Gentle Review on Modern NLP Models"
collection: publications
category: manuscripts # must include feature
permalink: /publication/modern-nlp-reviews
excerpt: 'Review paper'
date: 2024-10-20
# venue: 'Journal 1'
# slidesurl: 'http://academicpages.github.io/files/slides2.pdf'
paperurl: 'https://toobarahimnia.github.io/PersonalHub/files/NLP_Paper.pdf'
# citation: 'Rahimnia, Tooba. (2024). &quot;Paper Title Number 2.&quot; <i>Journal 1</i>. 1(2).'
---

This paper provides an overview of key models in natural language processing (NLP), focusing on their architectures, applications, and limitations. We discuss foundational models such as GPT, BERT, and T5, highlighting their contributions to the field and examining their strengths and challenges. Additionally, we explore the transition of transformer models from NLP to computer vision, exemplified by the Vision Transformer (ViT), showcasing the versatility of self-attention mechanisms across domains. This guide aims to offer a concise, accessible resource for those new to NLP, providing a solid foundation for understanding the current landscape of foundational models.